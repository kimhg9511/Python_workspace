{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x24cf1bcbf90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[1], \n",
    "                             [2], \n",
    "                             [3]]) # 입력\n",
    "y_train = torch.FloatTensor([[2], \n",
    "                             [4], \n",
    "                             [6]]) # 레이블 / 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.zeros(1, requires_grad=True)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.], requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.zeros(1, requires_grad=True)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis = x_train * W + b\n",
    "hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(18.6667, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD([W, b], lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "cost.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/3000 W: 0.187, b: 0.080 Cost: 18.666666\n",
      "Epoch  100/3000 W: 1.746, b: 0.578 Cost: 0.048171\n",
      "Epoch  200/3000 W: 1.800, b: 0.454 Cost: 0.029767\n",
      "Epoch  300/3000 W: 1.843, b: 0.357 Cost: 0.018394\n",
      "Epoch  400/3000 W: 1.876, b: 0.281 Cost: 0.011366\n",
      "Epoch  500/3000 W: 1.903, b: 0.221 Cost: 0.007024\n",
      "Epoch  600/3000 W: 1.924, b: 0.174 Cost: 0.004340\n",
      "Epoch  700/3000 W: 1.940, b: 0.136 Cost: 0.002682\n",
      "Epoch  800/3000 W: 1.953, b: 0.107 Cost: 0.001657\n",
      "Epoch  900/3000 W: 1.963, b: 0.084 Cost: 0.001024\n",
      "Epoch 1000/3000 W: 1.971, b: 0.066 Cost: 0.000633\n",
      "Epoch 1100/3000 W: 1.977, b: 0.052 Cost: 0.000391\n",
      "Epoch 1200/3000 W: 1.982, b: 0.041 Cost: 0.000242\n",
      "Epoch 1300/3000 W: 1.986, b: 0.032 Cost: 0.000149\n",
      "Epoch 1400/3000 W: 1.989, b: 0.025 Cost: 0.000092\n",
      "Epoch 1500/3000 W: 1.991, b: 0.020 Cost: 0.000057\n",
      "Epoch 1600/3000 W: 1.993, b: 0.016 Cost: 0.000035\n",
      "Epoch 1700/3000 W: 1.995, b: 0.012 Cost: 0.000022\n",
      "Epoch 1800/3000 W: 1.996, b: 0.010 Cost: 0.000013\n",
      "Epoch 1900/3000 W: 1.997, b: 0.008 Cost: 0.000008\n",
      "Epoch 2000/3000 W: 1.997, b: 0.006 Cost: 0.000005\n",
      "Epoch 2100/3000 W: 1.998, b: 0.005 Cost: 0.000003\n",
      "Epoch 2200/3000 W: 1.998, b: 0.004 Cost: 0.000002\n",
      "Epoch 2300/3000 W: 1.999, b: 0.003 Cost: 0.000001\n",
      "Epoch 2400/3000 W: 1.999, b: 0.002 Cost: 0.000001\n",
      "Epoch 2500/3000 W: 1.999, b: 0.002 Cost: 0.000000\n",
      "Epoch 2600/3000 W: 1.999, b: 0.001 Cost: 0.000000\n",
      "Epoch 2700/3000 W: 2.000, b: 0.001 Cost: 0.000000\n",
      "Epoch 2800/3000 W: 2.000, b: 0.001 Cost: 0.000000\n",
      "Epoch 2900/3000 W: 2.000, b: 0.001 Cost: 0.000000\n",
      "Epoch 3000/3000 W: 2.000, b: 0.001 Cost: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "# 모델 초기화\n",
    "W = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=0.01)\n",
    "\n",
    "nb_epochs = 3000 # 원하는만큼 경사 하강법을 반복\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    hypothesis = x_train * W + b\n",
    "\n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_train = torch.FloatTensor([[73], [93], [89], [96], [73]])\n",
    "x2_train = torch.FloatTensor([[80], [88], [91], [98], [66]])\n",
    "x3_train = torch.FloatTensor([[75], [93], [90], [100], [70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/10000 w1: 0.294 w2: 0.294 w3: 0.297 b: 0.003 Cost: 29661.800781\n",
      "Epoch  100/10000 w1: 0.674 w2: 0.661 w3: 0.676 b: 0.008 Cost: 1.563634\n",
      "Epoch  200/10000 w1: 0.679 w2: 0.655 w3: 0.677 b: 0.008 Cost: 1.497603\n",
      "Epoch  300/10000 w1: 0.684 w2: 0.649 w3: 0.677 b: 0.008 Cost: 1.435026\n",
      "Epoch  400/10000 w1: 0.689 w2: 0.643 w3: 0.678 b: 0.008 Cost: 1.375730\n",
      "Epoch  500/10000 w1: 0.694 w2: 0.638 w3: 0.678 b: 0.009 Cost: 1.319503\n",
      "Epoch  600/10000 w1: 0.699 w2: 0.633 w3: 0.679 b: 0.009 Cost: 1.266215\n",
      "Epoch  700/10000 w1: 0.704 w2: 0.627 w3: 0.679 b: 0.009 Cost: 1.215693\n",
      "Epoch  800/10000 w1: 0.709 w2: 0.622 w3: 0.679 b: 0.009 Cost: 1.167821\n",
      "Epoch  900/10000 w1: 0.713 w2: 0.617 w3: 0.680 b: 0.009 Cost: 1.122419\n",
      "Epoch 1000/10000 w1: 0.718 w2: 0.613 w3: 0.680 b: 0.009 Cost: 1.079375\n",
      "Epoch 1100/10000 w1: 0.722 w2: 0.608 w3: 0.680 b: 0.009 Cost: 1.038569\n",
      "Epoch 1200/10000 w1: 0.727 w2: 0.603 w3: 0.681 b: 0.010 Cost: 0.999893\n",
      "Epoch 1300/10000 w1: 0.731 w2: 0.599 w3: 0.681 b: 0.010 Cost: 0.963217\n",
      "Epoch 1400/10000 w1: 0.735 w2: 0.595 w3: 0.681 b: 0.010 Cost: 0.928421\n",
      "Epoch 1500/10000 w1: 0.739 w2: 0.591 w3: 0.681 b: 0.010 Cost: 0.895448\n",
      "Epoch 1600/10000 w1: 0.743 w2: 0.586 w3: 0.682 b: 0.010 Cost: 0.864175\n",
      "Epoch 1700/10000 w1: 0.746 w2: 0.583 w3: 0.682 b: 0.010 Cost: 0.834503\n",
      "Epoch 1800/10000 w1: 0.750 w2: 0.579 w3: 0.682 b: 0.010 Cost: 0.806368\n",
      "Epoch 1900/10000 w1: 0.754 w2: 0.575 w3: 0.682 b: 0.010 Cost: 0.779692\n",
      "Epoch 2000/10000 w1: 0.757 w2: 0.571 w3: 0.682 b: 0.011 Cost: 0.754390\n",
      "Epoch 2100/10000 w1: 0.760 w2: 0.568 w3: 0.682 b: 0.011 Cost: 0.730365\n",
      "Epoch 2200/10000 w1: 0.764 w2: 0.564 w3: 0.682 b: 0.011 Cost: 0.707607\n",
      "Epoch 2300/10000 w1: 0.767 w2: 0.561 w3: 0.682 b: 0.011 Cost: 0.685989\n",
      "Epoch 2400/10000 w1: 0.770 w2: 0.558 w3: 0.682 b: 0.011 Cost: 0.665490\n",
      "Epoch 2500/10000 w1: 0.773 w2: 0.555 w3: 0.682 b: 0.011 Cost: 0.646035\n",
      "Epoch 2600/10000 w1: 0.776 w2: 0.552 w3: 0.682 b: 0.011 Cost: 0.627585\n",
      "Epoch 2700/10000 w1: 0.779 w2: 0.549 w3: 0.682 b: 0.012 Cost: 0.610050\n",
      "Epoch 2800/10000 w1: 0.782 w2: 0.546 w3: 0.682 b: 0.012 Cost: 0.593426\n",
      "Epoch 2900/10000 w1: 0.785 w2: 0.543 w3: 0.682 b: 0.012 Cost: 0.577643\n",
      "Epoch 3000/10000 w1: 0.788 w2: 0.541 w3: 0.682 b: 0.012 Cost: 0.562648\n",
      "Epoch 3100/10000 w1: 0.791 w2: 0.538 w3: 0.682 b: 0.012 Cost: 0.548429\n",
      "Epoch 3200/10000 w1: 0.793 w2: 0.535 w3: 0.682 b: 0.012 Cost: 0.534923\n",
      "Epoch 3300/10000 w1: 0.796 w2: 0.533 w3: 0.682 b: 0.012 Cost: 0.522093\n",
      "Epoch 3400/10000 w1: 0.798 w2: 0.530 w3: 0.682 b: 0.012 Cost: 0.509904\n",
      "Epoch 3500/10000 w1: 0.801 w2: 0.528 w3: 0.682 b: 0.012 Cost: 0.498317\n",
      "Epoch 3600/10000 w1: 0.803 w2: 0.526 w3: 0.681 b: 0.013 Cost: 0.487341\n",
      "Epoch 3700/10000 w1: 0.806 w2: 0.524 w3: 0.681 b: 0.013 Cost: 0.476888\n",
      "Epoch 3800/10000 w1: 0.808 w2: 0.522 w3: 0.681 b: 0.013 Cost: 0.466961\n",
      "Epoch 3900/10000 w1: 0.810 w2: 0.519 w3: 0.681 b: 0.013 Cost: 0.457528\n",
      "Epoch 4000/10000 w1: 0.812 w2: 0.517 w3: 0.681 b: 0.013 Cost: 0.448554\n",
      "Epoch 4100/10000 w1: 0.814 w2: 0.515 w3: 0.681 b: 0.013 Cost: 0.440042\n",
      "Epoch 4200/10000 w1: 0.817 w2: 0.514 w3: 0.680 b: 0.013 Cost: 0.431924\n",
      "Epoch 4300/10000 w1: 0.819 w2: 0.512 w3: 0.680 b: 0.013 Cost: 0.424216\n",
      "Epoch 4400/10000 w1: 0.821 w2: 0.510 w3: 0.680 b: 0.014 Cost: 0.416875\n",
      "Epoch 4500/10000 w1: 0.823 w2: 0.508 w3: 0.680 b: 0.014 Cost: 0.409898\n",
      "Epoch 4600/10000 w1: 0.825 w2: 0.507 w3: 0.679 b: 0.014 Cost: 0.403260\n",
      "Epoch 4700/10000 w1: 0.826 w2: 0.505 w3: 0.679 b: 0.014 Cost: 0.396934\n",
      "Epoch 4800/10000 w1: 0.828 w2: 0.503 w3: 0.679 b: 0.014 Cost: 0.390917\n",
      "Epoch 4900/10000 w1: 0.830 w2: 0.502 w3: 0.679 b: 0.014 Cost: 0.385188\n",
      "Epoch 5000/10000 w1: 0.832 w2: 0.500 w3: 0.678 b: 0.014 Cost: 0.379734\n",
      "Epoch 5100/10000 w1: 0.834 w2: 0.499 w3: 0.678 b: 0.014 Cost: 0.374533\n",
      "Epoch 5200/10000 w1: 0.835 w2: 0.497 w3: 0.678 b: 0.014 Cost: 0.369581\n",
      "Epoch 5300/10000 w1: 0.837 w2: 0.496 w3: 0.677 b: 0.015 Cost: 0.364857\n",
      "Epoch 5400/10000 w1: 0.839 w2: 0.495 w3: 0.677 b: 0.015 Cost: 0.360357\n",
      "Epoch 5500/10000 w1: 0.840 w2: 0.493 w3: 0.677 b: 0.015 Cost: 0.356064\n",
      "Epoch 5600/10000 w1: 0.842 w2: 0.492 w3: 0.676 b: 0.015 Cost: 0.351970\n",
      "Epoch 5700/10000 w1: 0.843 w2: 0.491 w3: 0.676 b: 0.015 Cost: 0.348059\n",
      "Epoch 5800/10000 w1: 0.845 w2: 0.490 w3: 0.676 b: 0.015 Cost: 0.344323\n",
      "Epoch 5900/10000 w1: 0.846 w2: 0.489 w3: 0.675 b: 0.015 Cost: 0.340758\n",
      "Epoch 6000/10000 w1: 0.848 w2: 0.488 w3: 0.675 b: 0.015 Cost: 0.337349\n",
      "Epoch 6100/10000 w1: 0.849 w2: 0.487 w3: 0.675 b: 0.016 Cost: 0.334108\n",
      "Epoch 6200/10000 w1: 0.851 w2: 0.486 w3: 0.674 b: 0.016 Cost: 0.330994\n",
      "Epoch 6300/10000 w1: 0.852 w2: 0.485 w3: 0.674 b: 0.016 Cost: 0.328023\n",
      "Epoch 6400/10000 w1: 0.853 w2: 0.484 w3: 0.674 b: 0.016 Cost: 0.325181\n",
      "Epoch 6500/10000 w1: 0.854 w2: 0.483 w3: 0.673 b: 0.016 Cost: 0.322455\n",
      "Epoch 6600/10000 w1: 0.856 w2: 0.482 w3: 0.673 b: 0.016 Cost: 0.319849\n",
      "Epoch 6700/10000 w1: 0.857 w2: 0.481 w3: 0.673 b: 0.016 Cost: 0.317357\n",
      "Epoch 6800/10000 w1: 0.858 w2: 0.480 w3: 0.672 b: 0.016 Cost: 0.314965\n",
      "Epoch 6900/10000 w1: 0.859 w2: 0.479 w3: 0.672 b: 0.016 Cost: 0.312671\n",
      "Epoch 7000/10000 w1: 0.861 w2: 0.478 w3: 0.671 b: 0.016 Cost: 0.310468\n",
      "Epoch 7100/10000 w1: 0.862 w2: 0.478 w3: 0.671 b: 0.017 Cost: 0.308362\n",
      "Epoch 7200/10000 w1: 0.863 w2: 0.477 w3: 0.671 b: 0.017 Cost: 0.306344\n",
      "Epoch 7300/10000 w1: 0.864 w2: 0.476 w3: 0.670 b: 0.017 Cost: 0.304394\n",
      "Epoch 7400/10000 w1: 0.865 w2: 0.476 w3: 0.670 b: 0.017 Cost: 0.302527\n",
      "Epoch 7500/10000 w1: 0.866 w2: 0.475 w3: 0.669 b: 0.017 Cost: 0.300735\n",
      "Epoch 7600/10000 w1: 0.867 w2: 0.474 w3: 0.669 b: 0.017 Cost: 0.299002\n",
      "Epoch 7700/10000 w1: 0.868 w2: 0.474 w3: 0.668 b: 0.017 Cost: 0.297341\n",
      "Epoch 7800/10000 w1: 0.869 w2: 0.473 w3: 0.668 b: 0.017 Cost: 0.295741\n",
      "Epoch 7900/10000 w1: 0.870 w2: 0.472 w3: 0.668 b: 0.017 Cost: 0.294189\n",
      "Epoch 8000/10000 w1: 0.871 w2: 0.472 w3: 0.667 b: 0.018 Cost: 0.292716\n",
      "Epoch 8100/10000 w1: 0.872 w2: 0.471 w3: 0.667 b: 0.018 Cost: 0.291277\n",
      "Epoch 8200/10000 w1: 0.873 w2: 0.471 w3: 0.666 b: 0.018 Cost: 0.289885\n",
      "Epoch 8300/10000 w1: 0.874 w2: 0.470 w3: 0.666 b: 0.018 Cost: 0.288558\n",
      "Epoch 8400/10000 w1: 0.875 w2: 0.470 w3: 0.665 b: 0.018 Cost: 0.287262\n",
      "Epoch 8500/10000 w1: 0.876 w2: 0.469 w3: 0.665 b: 0.018 Cost: 0.286012\n",
      "Epoch 8600/10000 w1: 0.877 w2: 0.469 w3: 0.665 b: 0.018 Cost: 0.284802\n",
      "Epoch 8700/10000 w1: 0.878 w2: 0.469 w3: 0.664 b: 0.018 Cost: 0.283638\n",
      "Epoch 8800/10000 w1: 0.879 w2: 0.468 w3: 0.664 b: 0.018 Cost: 0.282503\n",
      "Epoch 8900/10000 w1: 0.880 w2: 0.468 w3: 0.663 b: 0.018 Cost: 0.281416\n",
      "Epoch 9000/10000 w1: 0.880 w2: 0.467 w3: 0.663 b: 0.019 Cost: 0.280342\n",
      "Epoch 9100/10000 w1: 0.881 w2: 0.467 w3: 0.662 b: 0.019 Cost: 0.279314\n",
      "Epoch 9200/10000 w1: 0.882 w2: 0.467 w3: 0.662 b: 0.019 Cost: 0.278311\n",
      "Epoch 9300/10000 w1: 0.883 w2: 0.466 w3: 0.661 b: 0.019 Cost: 0.277333\n",
      "Epoch 9400/10000 w1: 0.884 w2: 0.466 w3: 0.661 b: 0.019 Cost: 0.276393\n",
      "Epoch 9500/10000 w1: 0.884 w2: 0.466 w3: 0.661 b: 0.019 Cost: 0.275469\n",
      "Epoch 9600/10000 w1: 0.885 w2: 0.465 w3: 0.660 b: 0.019 Cost: 0.274575\n",
      "Epoch 9700/10000 w1: 0.886 w2: 0.465 w3: 0.660 b: 0.019 Cost: 0.273710\n",
      "Epoch 9800/10000 w1: 0.887 w2: 0.465 w3: 0.659 b: 0.019 Cost: 0.272855\n",
      "Epoch 9900/10000 w1: 0.887 w2: 0.464 w3: 0.659 b: 0.019 Cost: 0.272031\n",
      "Epoch 10000/10000 w1: 0.888 w2: 0.464 w3: 0.658 b: 0.020 Cost: 0.271221\n"
     ]
    }
   ],
   "source": [
    "w1 = torch.zeros(1, requires_grad=True)\n",
    "w2 = torch.zeros(1, requires_grad=True)\n",
    "w3 = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "optimizer = optim.SGD([w1, w2, w3, b], lr=0.00001)\n",
    "\n",
    "nb_epochs = 10000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b\n",
    "\n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} w1: {:.3f} w2: {:.3f} w3: {:.3f} b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, w1.item(), w2.item(), w3.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train  =  torch.FloatTensor([[73,  80,  75], \n",
    "                               [93,  88,  93], \n",
    "                               [89,  91,  90], \n",
    "                               [96,  98,  100],   \n",
    "                               [73,  66,  70]])  \n",
    "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
      "Epoch    1/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
      "Epoch    2/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
      "Epoch    3/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
      "Epoch    4/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
      "Epoch    5/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
      "Epoch    6/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
      "Epoch    7/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
      "Epoch    8/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
      "Epoch    9/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
      "Epoch   10/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
      "Epoch   11/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
      "Epoch   12/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
      "Epoch   13/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
      "Epoch   14/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
      "Epoch   15/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
      "Epoch   16/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
      "Epoch   17/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
      "Epoch   18/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
      "Epoch   19/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
      "Epoch   20/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n"
     ]
    }
   ],
   "source": [
    "W = torch.zeros((3, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) \n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    # 편향 b는 브로드 캐스팅되어 각 샘플에 더해집니다.\n",
    "    hypothesis = x_train.matmul(W) + b\n",
    "\n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(\n",
    "        epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "model = nn.Linear(3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.0335,  0.5347, -0.1827]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.3583], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/3000 Cost: 2.648077\n",
      "Epoch  100/3000 Cost: 2.518195\n",
      "Epoch  200/3000 Cost: 2.395159\n",
      "Epoch  300/3000 Cost: 2.278622\n",
      "Epoch  400/3000 Cost: 2.168202\n",
      "Epoch  500/3000 Cost: 2.063622\n",
      "Epoch  600/3000 Cost: 1.964560\n",
      "Epoch  700/3000 Cost: 1.870717\n",
      "Epoch  800/3000 Cost: 1.781821\n",
      "Epoch  900/3000 Cost: 1.697598\n",
      "Epoch 1000/3000 Cost: 1.617857\n",
      "Epoch 1100/3000 Cost: 1.542277\n",
      "Epoch 1200/3000 Cost: 1.470711\n",
      "Epoch 1300/3000 Cost: 1.402906\n",
      "Epoch 1400/3000 Cost: 1.338680\n",
      "Epoch 1500/3000 Cost: 1.277836\n",
      "Epoch 1600/3000 Cost: 1.220218\n",
      "Epoch 1700/3000 Cost: 1.165608\n",
      "Epoch 1800/3000 Cost: 1.113896\n",
      "Epoch 1900/3000 Cost: 1.064907\n",
      "Epoch 2000/3000 Cost: 1.018478\n",
      "Epoch 2100/3000 Cost: 0.974521\n",
      "Epoch 2200/3000 Cost: 0.932881\n",
      "Epoch 2300/3000 Cost: 0.893435\n",
      "Epoch 2400/3000 Cost: 0.856060\n",
      "Epoch 2500/3000 Cost: 0.820659\n",
      "Epoch 2600/3000 Cost: 0.787125\n",
      "Epoch 2700/3000 Cost: 0.755350\n",
      "Epoch 2800/3000 Cost: 0.725255\n",
      "Epoch 2900/3000 Cost: 0.696751\n",
      "Epoch 3000/3000 Cost: 0.669738\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) \n",
    "nb_epochs = 3000\n",
    "for epoch in range(nb_epochs+1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    # model(x_train)은 model.forward(x_train)와 동일함.\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n",
    "\n",
    "    # cost로 H(x) 개선하는 부분\n",
    "    # gradient를 0으로 초기화\n",
    "    optimizer.zero_grad()\n",
    "    # 비용 함수를 미분하여 gradient 계산\n",
    "    cost.backward()\n",
    "    # W와 b를 업데이트\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "    # 100번마다 로그 출력\n",
    "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "          epoch, nb_epochs, cost.item()\n",
    "      ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1,1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 Cost: 13.103541\n",
      "Epoch  100/2000 Cost: 0.002791\n",
      "Epoch  200/2000 Cost: 0.001724\n",
      "Epoch  300/2000 Cost: 0.001066\n",
      "Epoch  400/2000 Cost: 0.000658\n",
      "Epoch  500/2000 Cost: 0.000407\n",
      "Epoch  600/2000 Cost: 0.000251\n",
      "Epoch  700/2000 Cost: 0.000155\n",
      "Epoch  800/2000 Cost: 0.000096\n",
      "Epoch  900/2000 Cost: 0.000059\n",
      "Epoch 1000/2000 Cost: 0.000037\n",
      "Epoch 1100/2000 Cost: 0.000023\n",
      "Epoch 1200/2000 Cost: 0.000014\n",
      "Epoch 1300/2000 Cost: 0.000009\n",
      "Epoch 1400/2000 Cost: 0.000005\n",
      "Epoch 1500/2000 Cost: 0.000003\n",
      "Epoch 1600/2000 Cost: 0.000002\n",
      "Epoch 1700/2000 Cost: 0.000001\n",
      "Epoch 1800/2000 Cost: 0.000001\n",
      "Epoch 1900/2000 Cost: 0.000000\n",
      "Epoch 2000/2000 Cost: 0.000000\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegressionModel()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs+1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n",
    "\n",
    "    # cost로 H(x) 개선하는 부분\n",
    "    # gradient를 0으로 초기화\n",
    "    optimizer.zero_grad()\n",
    "    # 비용 함수를 미분하여 gradient 계산\n",
    "    cost.backward() # backward 연산\n",
    "    # W와 b를 업데이트\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "    # 100번마다 로그 출력\n",
    "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "          epoch, nb_epochs, cost.item()\n",
    "      ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
