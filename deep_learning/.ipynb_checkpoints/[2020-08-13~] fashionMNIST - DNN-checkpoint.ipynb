{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version: 1.5.0+cpu\n",
      "GPU 사용 가능 여부: False\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# import check_util.checker as checker \n",
    "\n",
    "print('pytorch version: {}'.format(torch.__version__))\n",
    "print('GPU 사용 가능 여부: {}'.format(torch.cuda.is_available()))\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_epochs = 5\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "root = './data'\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=(0.5,), std=(0.5,))])\n",
    "train_data = dset.FashionMNIST(root=root, train=True, transform=transform, download=True)\n",
    "test_data = dset.FashionMNIST(root=root, train=False, transform=transform, download=True)\n",
    "## 코드 시작 ##\n",
    "train_loader = DataLoader(dataset=train_data, \n",
    "                          batch_size=32, \n",
    "                          shuffle=True, \n",
    "                          num_workers=2)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_data, \n",
    "                          batch_size=32, \n",
    "                          shuffle=True, \n",
    "                          num_workers=2)\n",
    "## 코드 종료 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'checker' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-2f2ee590ea20>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mchecker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_loader_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'checker' is not defined"
     ]
    }
   ],
   "source": [
    "checker.train_loader_check(train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = {0 : 'T-Shirt', 1 : 'Trouser', 2 : 'Pullover', 3 : 'Dress', 4 : 'Coat', 5 : 'Sandal', 6 : 'Shirt',\n",
    "              7 : 'Sneaker', 8 : 'Bag', 9 : 'Ankle Boot'}\n",
    "columns = 5\n",
    "rows = 5\n",
    "# fig = plt.figure(figsize=(8,8))\n",
    "\n",
    "# for i in range(1, columns*rows+1):\n",
    "#     data_idx = np.random.randint(len(train_data))\n",
    "#     img = train_data[data_idx][0][0,:,:].numpy() # numpy()를 통해 torch Tensor를 numpy array로 변환\n",
    "#     label = labels_map[train_data[data_idx][1]] # item()을 통해 torch Tensor를 숫자로 변환\n",
    "    \n",
    "#     fig.add_subplot(rows, columns, i)\n",
    "#     plt.title(label)\n",
    "# #     plt.imshow(img, cmap='gray')\n",
    "#     plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(DNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(512, 128),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(128, 32),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Linear(32,10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1) # flatten\n",
    "        x_out = self.layer1(x)\n",
    "        x_out = self.layer2(x_out)\n",
    "        x_out = self.layer3(x_out)\n",
    "        x_out = self.layer4(x_out)\n",
    "\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear): # 모델의 모든 MLP 레이어에 대해서\n",
    "        nn.init.xavier_normal_(m.weight) # Weight를 xavier_normal로 초기화\n",
    "        print(m.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0437, -0.0047,  0.0671,  ..., -0.0125, -0.0129, -0.0146],\n",
      "        [ 0.0727, -0.0370,  0.0367,  ...,  0.0184,  0.0512,  0.0268],\n",
      "        [-0.0182,  0.0375, -0.0141,  ...,  0.0072, -0.0228,  0.0097],\n",
      "        ...,\n",
      "        [-0.0409,  0.0340, -0.0123,  ...,  0.0333,  0.0048,  0.0379],\n",
      "        [ 0.0204, -0.0544, -0.0059,  ...,  0.0047, -0.0267,  0.0256],\n",
      "        [ 0.0169,  0.0197, -0.0354,  ...,  0.0029, -0.0067, -0.0555]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0367,  0.0610, -0.1070,  ...,  0.0567,  0.0033,  0.0320],\n",
      "        [ 0.0578, -0.0620,  0.0097,  ...,  0.0025, -0.0586, -0.0876],\n",
      "        [-0.0626,  0.0149,  0.0781,  ...,  0.0642, -0.1264, -0.0073],\n",
      "        ...,\n",
      "        [ 0.0134,  0.0056,  0.1168,  ...,  0.0135,  0.0050,  0.1229],\n",
      "        [ 0.0017, -0.0512, -0.0223,  ..., -0.1189,  0.0555, -0.0026],\n",
      "        [-0.0321, -0.0273,  0.0709,  ..., -0.1396,  0.0144, -0.0953]],\n",
      "       requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNN(\n",
       "  (layer1): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(7777) # 일관된 weight initialization을 위한 random seed 설정\n",
    "model = DNN().to(device)\n",
    "model.apply(weights_init) # 모델에 weight_init 함수를 적용하여 weight를 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1d9a13c1108>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/1875], Loss: 0.8472, Accuracy: 71.88%\n",
      "Epoch [1/5], Step [200/1875], Loss: 0.4487, Accuracy: 81.25%\n",
      "Epoch [1/5], Step [300/1875], Loss: 0.5730, Accuracy: 81.25%\n",
      "Epoch [1/5], Step [400/1875], Loss: 0.2330, Accuracy: 93.75%\n",
      "Epoch [1/5], Step [500/1875], Loss: 0.3806, Accuracy: 87.50%\n",
      "Epoch [1/5], Step [600/1875], Loss: 0.4888, Accuracy: 78.12%\n",
      "Epoch [1/5], Step [700/1875], Loss: 0.5595, Accuracy: 78.12%\n",
      "Epoch [1/5], Step [800/1875], Loss: 0.1720, Accuracy: 96.88%\n",
      "Epoch [1/5], Step [900/1875], Loss: 0.2987, Accuracy: 84.38%\n",
      "Epoch [1/5], Step [1000/1875], Loss: 0.3378, Accuracy: 87.50%\n",
      "Epoch [1/5], Step [1100/1875], Loss: 0.3487, Accuracy: 93.75%\n",
      "Epoch [1/5], Step [1200/1875], Loss: 0.2891, Accuracy: 87.50%\n",
      "Epoch [1/5], Step [1300/1875], Loss: 0.2977, Accuracy: 84.38%\n",
      "Epoch [1/5], Step [1400/1875], Loss: 0.2572, Accuracy: 87.50%\n",
      "Epoch [1/5], Step [1500/1875], Loss: 0.3189, Accuracy: 84.38%\n",
      "Epoch [1/5], Step [1600/1875], Loss: 0.2466, Accuracy: 96.88%\n",
      "Epoch [1/5], Step [1700/1875], Loss: 0.2475, Accuracy: 93.75%\n",
      "Epoch [1/5], Step [1800/1875], Loss: 0.3227, Accuracy: 87.50%\n",
      "Epoch [2/5], Step [100/1875], Loss: 0.2835, Accuracy: 87.50%\n",
      "Epoch [2/5], Step [200/1875], Loss: 0.5111, Accuracy: 75.00%\n",
      "Epoch [2/5], Step [300/1875], Loss: 0.3241, Accuracy: 90.62%\n",
      "Epoch [2/5], Step [400/1875], Loss: 0.4150, Accuracy: 84.38%\n",
      "Epoch [2/5], Step [500/1875], Loss: 0.2343, Accuracy: 90.62%\n",
      "Epoch [2/5], Step [600/1875], Loss: 0.2239, Accuracy: 90.62%\n",
      "Epoch [2/5], Step [700/1875], Loss: 0.4749, Accuracy: 90.62%\n",
      "Epoch [2/5], Step [800/1875], Loss: 0.1993, Accuracy: 93.75%\n",
      "Epoch [2/5], Step [900/1875], Loss: 0.2156, Accuracy: 90.62%\n",
      "Epoch [2/5], Step [1000/1875], Loss: 0.3243, Accuracy: 87.50%\n",
      "Epoch [2/5], Step [1100/1875], Loss: 0.2637, Accuracy: 90.62%\n",
      "Epoch [2/5], Step [1200/1875], Loss: 0.7937, Accuracy: 68.75%\n",
      "Epoch [2/5], Step [1300/1875], Loss: 0.3796, Accuracy: 84.38%\n",
      "Epoch [2/5], Step [1400/1875], Loss: 0.1831, Accuracy: 93.75%\n",
      "Epoch [2/5], Step [1500/1875], Loss: 0.5300, Accuracy: 81.25%\n",
      "Epoch [2/5], Step [1600/1875], Loss: 0.3803, Accuracy: 81.25%\n",
      "Epoch [2/5], Step [1700/1875], Loss: 0.2356, Accuracy: 87.50%\n",
      "Epoch [2/5], Step [1800/1875], Loss: 0.3986, Accuracy: 81.25%\n",
      "Epoch [3/5], Step [100/1875], Loss: 0.3282, Accuracy: 90.62%\n",
      "Epoch [3/5], Step [200/1875], Loss: 0.3460, Accuracy: 87.50%\n",
      "Epoch [3/5], Step [300/1875], Loss: 0.3514, Accuracy: 84.38%\n",
      "Epoch [3/5], Step [400/1875], Loss: 0.3326, Accuracy: 90.62%\n",
      "Epoch [3/5], Step [500/1875], Loss: 0.2124, Accuracy: 87.50%\n",
      "Epoch [3/5], Step [600/1875], Loss: 0.2611, Accuracy: 87.50%\n",
      "Epoch [3/5], Step [700/1875], Loss: 0.2771, Accuracy: 90.62%\n",
      "Epoch [3/5], Step [800/1875], Loss: 0.1372, Accuracy: 96.88%\n",
      "Epoch [3/5], Step [900/1875], Loss: 0.4600, Accuracy: 81.25%\n",
      "Epoch [3/5], Step [1000/1875], Loss: 0.3396, Accuracy: 93.75%\n",
      "Epoch [3/5], Step [1100/1875], Loss: 0.4921, Accuracy: 87.50%\n",
      "Epoch [3/5], Step [1200/1875], Loss: 0.3653, Accuracy: 90.62%\n",
      "Epoch [3/5], Step [1300/1875], Loss: 0.1574, Accuracy: 93.75%\n",
      "Epoch [3/5], Step [1400/1875], Loss: 0.4106, Accuracy: 84.38%\n",
      "Epoch [3/5], Step [1500/1875], Loss: 0.4433, Accuracy: 87.50%\n",
      "Epoch [3/5], Step [1600/1875], Loss: 0.3674, Accuracy: 87.50%\n",
      "Epoch [3/5], Step [1700/1875], Loss: 0.1828, Accuracy: 93.75%\n",
      "Epoch [3/5], Step [1800/1875], Loss: 0.3040, Accuracy: 87.50%\n",
      "Epoch [4/5], Step [100/1875], Loss: 0.5242, Accuracy: 78.12%\n",
      "Epoch [4/5], Step [200/1875], Loss: 0.2707, Accuracy: 90.62%\n",
      "Epoch [4/5], Step [300/1875], Loss: 0.2031, Accuracy: 93.75%\n",
      "Epoch [4/5], Step [400/1875], Loss: 0.4473, Accuracy: 78.12%\n",
      "Epoch [4/5], Step [500/1875], Loss: 0.3872, Accuracy: 87.50%\n",
      "Epoch [4/5], Step [600/1875], Loss: 0.2572, Accuracy: 90.62%\n",
      "Epoch [4/5], Step [700/1875], Loss: 0.2728, Accuracy: 87.50%\n",
      "Epoch [4/5], Step [800/1875], Loss: 0.1426, Accuracy: 96.88%\n",
      "Epoch [4/5], Step [900/1875], Loss: 0.3634, Accuracy: 93.75%\n",
      "Epoch [4/5], Step [1000/1875], Loss: 0.1020, Accuracy: 96.88%\n",
      "Epoch [4/5], Step [1100/1875], Loss: 0.3552, Accuracy: 87.50%\n",
      "Epoch [4/5], Step [1200/1875], Loss: 0.3009, Accuracy: 87.50%\n",
      "Epoch [4/5], Step [1300/1875], Loss: 0.2107, Accuracy: 96.88%\n",
      "Epoch [4/5], Step [1400/1875], Loss: 0.2626, Accuracy: 90.62%\n",
      "Epoch [4/5], Step [1500/1875], Loss: 0.1708, Accuracy: 100.00%\n",
      "Epoch [4/5], Step [1600/1875], Loss: 0.3110, Accuracy: 90.62%\n",
      "Epoch [4/5], Step [1700/1875], Loss: 0.2164, Accuracy: 96.88%\n",
      "Epoch [4/5], Step [1800/1875], Loss: 0.1952, Accuracy: 90.62%\n",
      "Epoch [5/5], Step [100/1875], Loss: 0.4137, Accuracy: 81.25%\n",
      "Epoch [5/5], Step [200/1875], Loss: 0.1605, Accuracy: 93.75%\n",
      "Epoch [5/5], Step [300/1875], Loss: 0.1110, Accuracy: 100.00%\n",
      "Epoch [5/5], Step [400/1875], Loss: 0.1228, Accuracy: 96.88%\n",
      "Epoch [5/5], Step [500/1875], Loss: 0.3720, Accuracy: 87.50%\n",
      "Epoch [5/5], Step [600/1875], Loss: 0.1321, Accuracy: 96.88%\n",
      "Epoch [5/5], Step [700/1875], Loss: 0.2027, Accuracy: 87.50%\n",
      "Epoch [5/5], Step [800/1875], Loss: 0.4094, Accuracy: 87.50%\n",
      "Epoch [5/5], Step [900/1875], Loss: 0.3658, Accuracy: 81.25%\n",
      "Epoch [5/5], Step [1000/1875], Loss: 0.2150, Accuracy: 93.75%\n",
      "Epoch [5/5], Step [1100/1875], Loss: 0.3858, Accuracy: 78.12%\n",
      "Epoch [5/5], Step [1200/1875], Loss: 0.1173, Accuracy: 96.88%\n",
      "Epoch [5/5], Step [1300/1875], Loss: 0.2327, Accuracy: 93.75%\n",
      "Epoch [5/5], Step [1400/1875], Loss: 0.3328, Accuracy: 84.38%\n",
      "Epoch [5/5], Step [1500/1875], Loss: 0.1470, Accuracy: 93.75%\n",
      "Epoch [5/5], Step [1600/1875], Loss: 0.3054, Accuracy: 93.75%\n",
      "Epoch [5/5], Step [1700/1875], Loss: 0.5007, Accuracy: 81.25%\n",
      "Epoch [5/5], Step [1800/1875], Loss: 0.3770, Accuracy: 87.50%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (imgs, labels) in enumerate(train_loader):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, argmax = torch.max(outputs, 1)\n",
    "        accuracy = (labels == argmax).float().mean()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'.format(\n",
    "                epoch+1, num_epochs, i+1, len(train_loader), loss.item(), accuracy.item() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for 10000 images: 87.49%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (imgs, labels) in enumerate(test_loader):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model(imgs)\n",
    "        _, argmax = torch.max(outputs, 1) # max()를 통해 최종 출력이 가장 높은 class 선택\n",
    "        total += imgs.size(0)\n",
    "        correct += (labels == argmax).sum().item()\n",
    "    \n",
    "    print('Test accuracy for {} images: {:.2f}%'.format(total, correct / total * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = 5\n",
    "rows = 5\n",
    "# fig = plt.figure(figsize=(8,8))\n",
    "\n",
    "model.eval()\n",
    "for i in range(1, columns*rows+1):\n",
    "    data_idx = np.random.randint(len(test_data))\n",
    "    input_img = test_data[data_idx][0].unsqueeze(dim=0).to(device) \n",
    "    '''\n",
    "    unsqueeze()를 통해 입력 이미지의 shape을 (1, 28, 28)에서 (1, 1, 28, 28)로 변환. \n",
    "    모델에 들어가는 입력 이미지의 shape은 (batch_size, channel, width, height) 되어야 함에 주의하세요!\n",
    "    '''\n",
    "    output = model(input_img)\n",
    "    _, argmax = torch.max(output, 1)\n",
    "    pred = labels_map[argmax.item()]\n",
    "    label = labels_map[test_data[data_idx][1]]\n",
    "    \n",
    "#     fig.add_subplot(rows, columns, i)\n",
    "#     if pred == label:\n",
    "#         plt.title(pred + '(O)')\n",
    "#     else:\n",
    "#         plt.title(pred + '(X)' + ' / ' + label)\n",
    "#     plot_img = test_data[data_idx][0][0,:,:]\n",
    "#     plt.imshow(plot_img, cmap='gray')\n",
    "#     plt.axis('off')\n",
    "# model.train()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
